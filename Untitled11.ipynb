{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olpHZndbBB0h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBtjTdsxM29E"
      },
      "outputs": [],
      "source": [
        "!unzip -u \"/content/drive/MyDrive/ensemble.zip\" -d \"/content/TRAFFIC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml6ZFAbhM0Bm"
      },
      "outputs": [],
      "source": [
        "!pip install PyTorch\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ds2NoBrMN9R",
        "outputId": "9bf466d4-3bd0-4e17-ae02-725e33d21982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1080, 1920) (1080, 1920) (1080, 1920)\n",
            "torch.Size([1080, 1920]) torch.Size([1080, 1920]) torch.Size([1080, 1920])\n",
            "tensor([[0.6911, 0.0379, 0.2711]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(13839.9727, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-0c2846b1d33d>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.softmax(x)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1080, 1920])) that is different to the input size (torch.Size([1, 1080, 1920])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import os\n",
        "import cv2\n",
        "# from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Load the ViT model\n",
        "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)  # this loads a pretrained version\n",
        "for parameter in vit_model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# Store the in_features from the original classifier\n",
        "in_features = vit_model.head.in_features\n",
        "\n",
        "# Replace the classifier with nn.Identity\n",
        "vit_model.head = nn.Identity()\n",
        "\n",
        "# Your model that utilizes the loaded ViT\n",
        "class ViTForWeights(nn.Module):\n",
        "    def __init__(self, vit_model, in_features):\n",
        "        super(ViTForWeights, self).__init__()\n",
        "        self.vit = vit_model\n",
        "        self.fc = nn.Linear(in_features, 3)  # We're using the stored in_features value here\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vit(x)\n",
        "        x = self.fc(x)\n",
        "        out = self.softmax(x)\n",
        "        return out  # Use softmax to ensure the weights sum up to 1\n",
        "\n",
        "model = ViTForWeights(vit_model, in_features) # Use softmax to ensure the weights sum up to 1\n",
        "# summary(model.cuda(), input_size=(3, 224, 224))\n",
        "\n",
        "# Example images\n",
        "bgs_path = \"/content/TRAFFIC/ensemble/Subsense/\"\n",
        "of_path = \"/content/TRAFFIC/ensemble/RLOF/\"\n",
        "suim_path = \"/content/TRAFFIC/ensemble/SUIM/\"\n",
        "\n",
        "# read the images from each folder\n",
        "bgs_images = os.listdir(bgs_path)\n",
        "of_images = os.listdir(of_path)\n",
        "suim_images = os.listdir(suim_path)\n",
        "\n",
        "# sort the images in ascending order based on the number in the file name\n",
        "bgs_images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "of_images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "suim_images.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "# read the first image to get the height and width\n",
        "bgs_image = cv2.imread(bgs_path + bgs_images[0], cv2.IMREAD_UNCHANGED)\n",
        "ht, wt = bgs_image.shape[:2]\n",
        "\n",
        "# go through each image and apply soft voting\n",
        "# for k in range(len(bgs_images)):\n",
        "bgs_image = cv2.imread(bgs_path + bgs_images[1], cv2.IMREAD_UNCHANGED)\n",
        "of_image = cv2.imread(of_path + of_images[1], cv2.IMREAD_UNCHANGED)\n",
        "suim_image = cv2.imread(suim_path + suim_images[1], cv2.IMREAD_UNCHANGED)\n",
        "suim_image = cv2.resize(suim_image, (wt, ht))\n",
        "print(bgs_image.shape,of_image.shape,suim_image.shape)\n",
        "\n",
        "# Concatenate along the channel dimension\n",
        "bgs_image = torch.tensor(bgs_image).float()\n",
        "of_image = torch.tensor(of_image).float()\n",
        "suim_image = torch.tensor(suim_image).float()\n",
        "print(bgs_image.size(),of_image.size(),suim_image.size())\n",
        "\n",
        "# Add a channel dimension to each image\n",
        "bgs_image = bgs_image.unsqueeze(0)\n",
        "of_image = of_image.unsqueeze(0)\n",
        "suim_image = suim_image.unsqueeze(0)\n",
        "\n",
        "# Concatenate along the channel dimension\n",
        "concat_images = torch.cat([bgs_image, of_image, suim_image], dim=0)  # Now the result will be of shape (3, height, width)\n",
        "# ci = concat_images.permute(1,2,0)\n",
        "# ci= ci.detach().numpy()\n",
        "# cv2.imwrite(\"out.jpg\",ci)\n",
        "# plt.imshow(ci)\n",
        "# plt.show()\n",
        "# print(ci.shape)\n",
        "# Add a batch dimension to the concatenated images\n",
        "concat_images = concat_images.unsqueeze(0)  # The result will be of shape (1, 3, height, width)\n",
        "\n",
        "# Resize concatenated image to match ViT input\n",
        "resize_size = (224, 224)\n",
        "concat_images_resized = F.interpolate(concat_images, size=resize_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "# Pass the resized image through the model\n",
        "weights = model(concat_images_resized)\n",
        "\n",
        "# # Now you can pass this to your model\n",
        "# weights = model(concat_images)\n",
        "\n",
        "# Multiply with the images\n",
        "print(weights)\n",
        "weighted_image1 = weights[:, 0].unsqueeze(1) * bgs_image\n",
        "weighted_image2 = weights[:, 1].unsqueeze(1) * of_image\n",
        "weighted_image3 = weights[:, 2].unsqueeze(1) * suim_image\n",
        "\n",
        "ground_truth = cv2.imread(\"/content/TRAFFIC/ensemble/groundtruth/\" + bgs_images[1], cv2.IMREAD_UNCHANGED)\n",
        "ground_truth = torch.tensor(ground_truth).float()\n",
        "# Calculate loss with ground truth\n",
        "# Assuming `ground_truth` is your ground truth tensor\n",
        "loss = nn.MSELoss()\n",
        "total_loss = loss(weighted_image1, ground_truth) + loss(weighted_image2, ground_truth) + loss(weighted_image3, ground_truth)\n",
        "\n",
        "print(total_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnANpry0Y7Ia"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# class diceloss(torch.nn.Module):\n",
        "#     def __init__(self, eps=1e-7):\n",
        "#         super(diceloss, self).__init__()\n",
        "#         self.eps = eps\n",
        "\n",
        "#     def forward(self, prediction, target):\n",
        "#         # prediction = F.sigmoid(prediction)\n",
        "\n",
        "#         intersection = (prediction * target).sum()\n",
        "#         union = prediction.sum() + target.sum()\n",
        "\n",
        "#         dice = 2.0 * intersection / (union + self.eps)\n",
        "#         return 1.0 - dice\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-7):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        intersection = (prediction * target).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (prediction.sum() + target.sum() + self.smooth)\n",
        "        return 1.0 - dice\n",
        "class diceloss(torch.nn.Module):\n",
        "    def init(self):\n",
        "        super(diceloss, self).init()\n",
        "    def forward(self,pred, target):\n",
        "      lenIntersection=0\n",
        "      for i in range(pred.shape[0]):\n",
        "          for j in range(pred.shape[1]):\n",
        "              if ( np.array_equal(pred[i][j],target[i][j]) ):\n",
        "                  lenIntersection+=1\n",
        "\n",
        "      lenimg=pred.shape[1]*pred.shape[1]\n",
        "      lenimg2=target.shape[1]*target.shape[2]\n",
        "      value = (2. * lenIntersection  / (lenimg + lenimg2))\n",
        "      return value\n",
        "# class diceloss(torch.nn.Module):\n",
        "#     def init(self):\n",
        "#         super(diceloss, self).init()\n",
        "#     def forward(self,pred, target):\n",
        "#        smooth = 1.\n",
        "#        iflat = pred.contiguous().view(-1)\n",
        "#        tflat = target.contiguous().view(-1)\n",
        "#        intersection = (iflat * tflat).sum()\n",
        "#        A_sum = torch.sum(iflat * iflat)\n",
        "#        B_sum = torch.sum(tflat * tflat)\n",
        "#        return 1 - (((2. * intersection) + smooth) / (A_sum + B_sum + smooth) )\n",
        "model = ViTForWeights(vit_model, in_features)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "# loss_function = nn.MSELoss()\n",
        "model.train()\n",
        "resize_size = (224, 224)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for k in range(40):\n",
        "        optimizer.zero_grad()\n",
        "        bgs_image = cv2.imread(bgs_path + bgs_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        of_image = cv2.imread(of_path + of_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        suim_image = cv2.imread(suim_path + suim_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        of_image = cv2.threshold(of_image, 0.3*255, 255, cv2.THRESH_BINARY)[1]\n",
        "        suim_image = cv2.threshold(suim_image, 0.3*255, 255, cv2.THRESH_BINARY)[1]\n",
        "        ht, wt = bgs_image.shape[:2]\n",
        "        suim_image = cv2.resize(suim_image, (wt, ht))\n",
        "\n",
        "        bgs_image = torch.tensor(bgs_image)\n",
        "        of_image = torch.tensor(of_image)\n",
        "        suim_image = torch.tensor(suim_image)\n",
        "\n",
        "        bgs_image = bgs_image.unsqueeze(0)\n",
        "        of_image = of_image.unsqueeze(0)\n",
        "        suim_image = suim_image.unsqueeze(0)\n",
        "\n",
        "        concat_images = torch.cat([bgs_image, of_image, suim_image], dim=0).unsqueeze(0)\n",
        "        concat_images_resized = F.interpolate(concat_images, size=resize_size, mode='bilinear', align_corners=True)\n",
        "        # print(concat_images_resized.max(),concat_images_resized.size())\n",
        "        # asdsa\n",
        "        weights = model(concat_images_resized.float())\n",
        "\n",
        "        weighted_image = weights[:, 0] * bgs_image.squeeze() + weights[:, 1] * of_image.squeeze() + weights[:, 2] * suim_image.squeeze()\n",
        "        weighted_image = cv2.threshold(weighted_image.detach().numpy(),0.3,1,cv2.THRESH_BINARY)[1]\n",
        "        ground_truth = cv2.imread(\"/content/TRAFFIC/ensemble/groundtruth/\" + bgs_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        ground_truth = torch.tensor(ground_truth)\n",
        "        # Compute the losses\n",
        "        criterion = DiceLoss()\n",
        "        combined_loss = criterion(torch.from_numpy(weighted_image).requires_grad_(True), ground_truth)\n",
        "        # combined_loss = combined_loss.tensor(requires_grad=True)\n",
        "        # torch.tensor(combined_loss,requires_grad=True)\n",
        "        # print(combined_loss)\n",
        "        total_loss += combined_loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/40}\",weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import timm\n",
        "from torch import nn\n",
        "\n",
        "# Initialize ViT model\n",
        "vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "for parameter in vit_model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "in_features = vit_model.head.in_features\n",
        "vit_model.head = nn.Identity()\n",
        "\n",
        "class ViTForWeights(nn.Module):\n",
        "    def __init__(self, vit_model, in_features):\n",
        "        super(ViTForWeights, self).__init__()\n",
        "        self.vit = vit_model\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 3),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vit(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-5):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        intersection = (prediction * target).sum()\n",
        "        union = prediction.sum() + target.sum()\n",
        "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1.0 - dice\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = ViTForWeights(vit_model, in_features)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Decreased learning rate\n",
        "criterion = DiceLoss()\n",
        "\n",
        "resize_size = (224, 224)\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for k in range(40):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        bgs_image = cv2.imread(bgs_path + bgs_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        of_image = cv2.imread(of_path + of_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        suim_image = cv2.imread(suim_path + suim_images[k], cv2.IMREAD_UNCHANGED)\n",
        "        of_image = cv2.threshold(of_image, 0.3*255, 255, cv2.THRESH_BINARY)[1]\n",
        "        suim_image = cv2.threshold(suim_image, 0.3*255, 255, cv2.THRESH_BINARY)[1]\n",
        "        ht, wt = bgs_image.shape[:2]\n",
        "        suim_image = cv2.resize(suim_image, (wt, ht))\n",
        "\n",
        "        bgs_image = torch.tensor(bgs_image)\n",
        "        of_image = torch.tensor(of_image)\n",
        "        suim_image = torch.tensor(suim_image)\n",
        "\n",
        "        bgs_image = bgs_image.unsqueeze(0)\n",
        "        of_image = of_image.unsqueeze(0)\n",
        "        suim_image = suim_image.unsqueeze(0)\n",
        "        weights = model(concat_images_resized.float())\n",
        "\n",
        "        weighted_image = (weights[:, 0] * bgs_image.squeeze() +\n",
        "                          weights[:, 1] * of_image.squeeze() +\n",
        "                          weights[:, 2] * suim_image.squeeze())\n",
        "\n",
        "        weighted_image_thresholded = (weighted_image > 0.3).float()\n",
        "\n",
        "        loss = criterion(weighted_image_thresholded, ground_truth)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/40}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "MjshoZKEQaO9",
        "outputId": "f9ed019b-b931-4bf9-9f6d-d2270eca7222"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e87944ffb554>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import timm\n",
        "from torch import nn\n",
        "\n",
        "# ... [All the class and function definitions] ...\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ViTForWeights(vit_model, in_features).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "\n",
        "resize_size = (224, 224)\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for k in range(40):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        bgs_image = cv2.imread(bgs_path + bgs_images[k], cv2.IMREAD_UNCHANGED) / 255.0\n",
        "        of_image = cv2.imread(of_path + of_images[k], cv2.IMREAD_UNCHANGED) / 255.0\n",
        "        suim_image = cv2.imread(suim_path + suim_images[k], cv2.IMREAD_UNCHANGED) / 255.0\n",
        "\n",
        "        ht, wt = bgs_image.shape[:2]\n",
        "        suim_image = cv2.resize(suim_image, (wt, ht))\n",
        "\n",
        "        bgs_image = torch.tensor(bgs_image).unsqueeze(0).to(device)\n",
        "        of_image = torch.tensor(of_image).unsqueeze(0).to(device)\n",
        "        suim_image = torch.tensor(suim_image).unsqueeze(0).to(device)\n",
        "\n",
        "        concat_images = torch.cat([bgs_image, of_image, suim_image], dim=0).unsqueeze(0)\n",
        "        concat_images_resized = F.interpolate(concat_images, size=resize_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        weights = model(concat_images_resized.float())\n",
        "        weighted_image = weights[0, 0] * bgs_image + weights[0, 1] * of_image + weights[0, 2] * suim_image\n",
        "\n",
        "        ground_truth = cv2.imread(\"/content/TRAFFIC/ensemble/groundtruth/\" + bgs_images[k], cv2.IMREAD_UNCHANGED) / 255.0\n",
        "        ground_truth = torch.tensor(ground_truth).unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute the loss\n",
        "        combined_loss = criterion(weighted_image, ground_truth)\n",
        "\n",
        "        total_loss += combined_loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / 40}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hUqfWCMRD0",
        "outputId": "9580adbf-af59-44b8-f6aa-5e19a55f293d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-0c2846b1d33d>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 126.82596793333846\n",
            "Epoch 2/20, Loss: 126.59412537953752\n",
            "Epoch 3/20, Loss: 126.52288935464419\n",
            "Epoch 4/20, Loss: 126.48071374562008\n",
            "Epoch 5/20, Loss: 126.44642258142703\n",
            "Epoch 6/20, Loss: 126.41518349403911\n",
            "Epoch 7/20, Loss: 126.3859973744641\n",
            "Epoch 8/20, Loss: 126.35880434483916\n",
            "Epoch 9/20, Loss: 126.3336970619456\n",
            "Epoch 10/20, Loss: 126.31069025088479\n",
            "Epoch 11/20, Loss: 126.28970221264642\n",
            "Epoch 12/20, Loss: 126.270596515485\n",
            "Epoch 13/20, Loss: 126.25321624002531\n",
            "Epoch 14/20, Loss: 126.23740001826206\n",
            "Epoch 15/20, Loss: 126.22298975017807\n",
            "Epoch 16/20, Loss: 126.20983725189413\n",
            "Epoch 17/20, Loss: 126.19780509904831\n",
            "Epoch 18/20, Loss: 126.18676971893133\n",
            "Epoch 19/20, Loss: 126.1766199787894\n",
            "Epoch 20/20, Loss: 126.16725765963801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cv0NbRtMeGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}